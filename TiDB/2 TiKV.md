# 简介
在TiDB集群中负责数据落盘与管理维护的组件。同时为其他服务直接提供业务数据。

# 数据组织管理方式
 - TiKV选择了key-value的数据存储模型，将数据以`Region`为单位进行管理。每一个`Region`都是一个左闭右开的区间，保存了一段连续的`Key-Value Pairs`（键值对）。  
 - 数据的原本的`key`会与时间版本组装在一起逆向排序。即一个数据的多个版本放在一起，最新的数据放在前面。以此来实现`MVCC`（多版本并发控制 ，Multi-Version Concurrency Control）
 - 因为实现了MVCC，所以TiKV中的数据会有他的生命周期，也就会有GC。默认情况下为10分钟-10分钟。即历史数据保留10分钟，每10分钟清理一次。我们的生产环境为24h-10min。
 - 使用`RocksDB`为单机存储引擎，使用`Raft`协议进行数据的多副本管理。数据通过`Raft`协议写入多个节点，然后通过`RocksDB`最终以`SSTable`的方式落盘存储。
 - TiKV会定时向 PD 发送心跳和`Region`的`meta data`，并按照`PD`制定的调度计划进行数据的维护与管理。但本机节点的数据维护由`RocksDB`自己负责。

# TiKV的一些特性
 - TiKV会使用服务器的60%-75%的内存用来运行并缓存热点数据。
 - 为了遵循`Raft`协议，满足多副本的需求，每个`Region`有通常维持在满足replica数量的副本。每个副本被成为`Peer`。所以`Peer`是TiDB中数据的最小管理单位。
 - TiKV节点在运维时，跨节点的数据调度速率受 PD 生成调度计划的限制。同时每个TiKV节点还有自己的传入（`add-peer`）和传出（`remove-peer`）速率限制，他们同时生效，以最低的为主。
 - TiKV节点下线速度慢，很大程度上受到`Leader`驱逐速度慢影响，如果手动驱逐`Leader`，可以大大加快节点的下线速度。
 - 每个`Region`默认情况下为`96MiB`，最大或者最小都不超过50%，即最大`144MiB`，最小`48MiB`。超过限制大小就会触发`Region`的`Split`或者`Merge`。
 - 同样因为每个`Region`只有`96MiB`，在超大规模集群中，会有海量的`Region`导致管理的开销将会很大，节点数量过多的时候还可以从`Raft`管理开销进行优化。
 - Pingcap建议每个 TiKV 节点的存储规格不超过`4T`，每个磁盘使用不要超过80%，使SSD磁盘有足够的空间进行数据的读写操作。
 - TiKV数据保存在磁盘上时通常使用了`lz4`和`zstd`算法进行压缩。所以通常会有比较大的文件体积放大率（Size amplification）。所以当有需要全表扫描的场合将会使用非常的多的CPU资源。
