# 特性
- 一键水平扩缩容  
	得益于 TiDB 存储计算分离的架构的设计，可按需对计算、存储分别进行在线扩容或者缩容，扩容或者缩容过程中对应用运维人员透明。
- 金融级高可用  
    数据采用多副本存储，数据副本通过 Multi-Raft 协议同步事务日志，多数派写入成功事务才能提交，确保数据强一致性且少数副本发生故障时不影响数据的可用性。可按需配置副本地理位置、副本数量等策略，满足不同容灾级别的要求。
- 实时 HTAP  
    提供行存储引擎 [TiKV](https://docs.pingcap.com/zh/tidb/stable/tikv-overview/)、列存储引擎 [TiFlash](https://docs.pingcap.com/zh/tidb/stable/tiflash-overview/) 两款存储引擎，TiFlash 通过 Multi-Raft Learner 协议实时从 TiKV 复制数据，确保行存储引擎 TiKV 和列存储引擎 TiFlash 之间的数据强一致。TiKV、TiFlash 可按需部署在不同的机器，解决 HTAP 资源隔离的问题。
- 云原生的分布式数据库  
    专为云而设计的分布式数据库，通过 [TiDB Operator](https://docs.pingcap.com/zh/tidb-in-kubernetes/stable/tidb-operator-overview) 可在公有云、私有云、混合云中实现部署工具化、自动化。
- 兼容 MySQL 协议和 MySQL 生态  
    兼容 MySQL 协议、MySQL 常用的功能、MySQL 生态，应用无需或者修改少量代码即可从 MySQL 迁移到 TiDB。提供丰富的[数据迁移工具](https://docs.pingcap.com/zh/tidb/stable/ecosystem-tool-user-guide/)帮助应用便捷完成数据迁移。
# 优势
- 纯分布式架构，拥有良好的扩展性，支持弹性的扩缩容
- 支持 SQL，对外暴露 MySQL 的网络协议，并兼容大多数 MySQL 的语法，在大多数场景下可以直接替换 MySQL
- 默认支持高可用，在少数副本失效的情况下，数据库本身能够自动进行数据修复和故障转移，对业务透明
- 支持 ACID 事务，对于一些有强一致需求的场景友好，例如：银行转账
- 具有丰富的工具链生态，覆盖数据迁移、同步、备份等多种场景
# 主要组件
[TiDB 整体架构](https://docs.pingcap.com/zh/tidb/stable/tidb-architecture/)
## [[3 TiDB|TiDB]]
[SQL 层](https://docs.pingcap.com/zh/tidb/stable/tidb-computing/#sql-%E5%B1%82%E7%AE%80%E4%BB%8B)，对外暴露 MySQL 协议的连接 endpoint，负责接受客户端的连接，执行 SQL 解析和优化，最终生成分布式执行计划。  
TiDB 层本身是无状态的，实践中可以启动多个 TiDB 实例，通过负载均衡组件（如 TiProxy、LVS、HAProxy、ProxySQL 或 F5）对外提供统一的接入地址，客户端的连接可以均匀地分摊在多个 TiDB 实例上以达到负载均衡的效果。  
TiDB Server 本身并不存储数据，只是解析 SQL，将实际的数据读取请求转发给底层的存储节点 TiKV（或 TiFlash）。
## [[1 PD|PD]]
整个 TiDB 集群的元信息管理模块，负责存储每个 TiKV 节点实时的数据分布情况和集群的整体拓扑结构，提供 TiDB Dashboard 管控界面，并为分布式事务分配事务 ID（包括TSO）。  
PD 不仅存储元信息，同时也负责集群数据的[实时调度](https://docs.pingcap.com/zh/tidb/stable/tidb-scheduling/)。根据 TiKV 节点实时上报的数据分布状态，生成调度计划下发调度命令给具体的 TiKV 节点。  
其自身内部使用ETCD存储和同步数据。还拥有一个可以观察整个集群状态的dashboard。
## 存储
[TiDB 数据库的存储](https://docs.pingcap.com/zh/tidb/stable/tidb-storage/)包括TiKV和TiFlash两种。
### [[2 TiKV|TiKV]]
底层是用RocksDB实现的分布式的提供事务的 Key-Value 存储引擎。  
存储数据的基本单位是 Region，每个 Region 负责存储一个 Key Range（从 StartKey 到 EndKey 的左闭右开区间）的数据，每个 TiKV 节点会负责多个 Region。Region使用raft协议进行同步和管理，其中Peer是Region的更小单位，是Region的副本（包括主副）。  
TiKV 中的数据都会自动维护多副本（默认为三副本），天然支持高可用和自动故障转移。同时也会根据PD生成调度协议在不同的节点之间互相交换数据，比如热点读写负载均衡。  
以SST的方式存储和管理数据落盘，具有较高的压缩放大比率。（3~5倍）
### TiFlash
TiFlash 是列存储节点，主要的功能是为分析型的场景加速。可以以存算分离的方式进行部署。

## 其他
### [[4 TiCDC|TiCDC]]
[TiDB 增量数据同步工具](https://docs.pingcap.com/zh/tidb/stable/ticdc-overview/)，通过拉取上游 TiKV 的数据变更日志，TiCDC 可以将数据解析为有序的行级变更数据输出到下游。  
适用于以下场景：
- 提供多 TiDB 集群，跨区域数据高可用和容灾方案，保证在灾难发生时保证主备集群数据的最终一致性。
- 提供同步实时变更数据到异构系统的服务，为监控、缓存、全文索引、数据分析、异构数据库使用等场景提供数据源。
# 部署
TiDB专为分布式高性能设计，其硬件要求较高。对Linux环境要求较低，软件需求中大多数为[性能优化](https://docs.pingcap.com/zh/tidb/stable/check-before-deployment/)考虑。  
使用[TiUP工具部署](https://docs.pingcap.com/zh/tidb/stable/production-deployment-using-tiup/)简单快捷，当环境不允许的时候亦可以进行离线部署。  
部署时使用 [yaml 文件](https://docs.pingcap.com/zh/tidb/stable/minimal-deployment-topology/)描述集群的状态，通过TiUP的集群命令可以一键部署。  
## 各类型节点对硬件的需求
 - TiDB需要高CPU高网络性能，根据SQL的类型或许会需要大内存节点。
 - PD对硬件的需求很低，但需要较好的网络性能，因为所有节点都会通过PD交换管理信息。
 - TiKV会固定使用60%到75%的内存作为热点数据缓存，所以根据业务类型需求选择节点的内存规模。同时冷数据存取时候会消耗一些的CPU性能，这也是需要考量的因素。
 - TiCDC需要适量的内存，较高的CPU。当下游集群是Mysql或者TiDB的时候需要很好的磁盘性能，因为TiCDC从上游的TiKV拉取完数据后会存放在本地进行预处理，然后再输出到下游。
 ## 推荐的部署方案
  - TiDB部署在Kubernetes中，具有良好的弹性。
  - PD部署在稳定的任意环境即可。
  - TiKV和TiCDC使用规格合适的物理机以及高性能磁盘。（磁盘性能够好的话，虚拟机也可以）
